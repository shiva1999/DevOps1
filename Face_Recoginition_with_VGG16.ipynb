{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Face_Recoginition with VGG16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfDfJMoUBgyw",
        "colab_type": "text"
      },
      "source": [
        "# Face Recognition using VGG16\n",
        "\n",
        "### Loading the VGG16 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwFZbgunBgzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
        "img_rows = 224\n",
        "img_cols = 224 \n",
        "\n",
        "#Loads the VGG16 model \n",
        "model = VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1hI2W2NBgz7",
        "colab_type": "text"
      },
      "source": [
        "### Inpsecting each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKgGcsQ1Bgz9",
        "colab_type": "code",
        "outputId": "f081ee57-6afb-429a-8cb2-a63dc8bb8541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(model.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D True\n",
            "2 Conv2D True\n",
            "3 MaxPooling2D True\n",
            "4 Conv2D True\n",
            "5 Conv2D True\n",
            "6 MaxPooling2D True\n",
            "7 Conv2D True\n",
            "8 Conv2D True\n",
            "9 Conv2D True\n",
            "10 MaxPooling2D True\n",
            "11 Conv2D True\n",
            "12 Conv2D True\n",
            "13 Conv2D True\n",
            "14 MaxPooling2D True\n",
            "15 Conv2D True\n",
            "16 Conv2D True\n",
            "17 Conv2D True\n",
            "18 MaxPooling2D True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r3yH8QGBg0E",
        "colab_type": "text"
      },
      "source": [
        "### Freezing of layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJQnNEfYBg0F",
        "colab_type": "code",
        "outputId": "461afa53-36bf-457e-c9f8-b539f07a3bf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
        "img_rows = 224\n",
        "img_cols = 224 \n",
        "\n",
        "# Re-loads the VGG16 model without the top or FC layers\n",
        "model = VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))\n",
        "\n",
        "# Here we freeze the last 4 layers \n",
        "# Layers are set to trainable as True by default\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(model.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D False\n",
            "2 Conv2D False\n",
            "3 MaxPooling2D False\n",
            "4 Conv2D False\n",
            "5 Conv2D False\n",
            "6 MaxPooling2D False\n",
            "7 Conv2D False\n",
            "8 Conv2D False\n",
            "9 Conv2D False\n",
            "10 MaxPooling2D False\n",
            "11 Conv2D False\n",
            "12 Conv2D False\n",
            "13 Conv2D False\n",
            "14 MaxPooling2D False\n",
            "15 Conv2D False\n",
            "16 Conv2D False\n",
            "17 Conv2D False\n",
            "18 MaxPooling2D False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCsKMOSbBg0M",
        "colab_type": "text"
      },
      "source": [
        "### Let's make a function that returns our FC Head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17HRE1NEBg0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def addTopModel(bottom_model, num_classes, D=256):\n",
        "    \"\"\"creates the top or head of the model that will be \n",
        "    placed ontop of the bottom layers\"\"\"\n",
        "    top_model = bottom_model.output\n",
        "    top_model = Flatten(name = \"flatten\")(top_model)\n",
        "    top_model = Dense(4096, activation = \"relu\")(top_model)\n",
        "    top_model = Dense(1024, activation = \"relu\")(top_model)\n",
        "    top_model = Dense(512, activation = \"relu\")(top_model)\n",
        "    top_model = Dense(256, activation = \"relu\")(top_model)\n",
        "    top_model = Dense(32, activation = \"relu\")(top_model)\n",
        "    top_model = Dropout(0.3)(top_model)\n",
        "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\n",
        "    return top_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjDdQqyoBg0b",
        "colab_type": "code",
        "outputId": "8f7915c6-420a-4a92-ddbb-667674e5da41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.input"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'input_4:0' shape=(None, 224, 224, 3) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6GKMf8UBg0l",
        "colab_type": "code",
        "outputId": "7a007862-db7b-4f6f-b6c0-0fc525dd1d63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "model.layers"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7f4117eec978>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117c19160>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117c13c18>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f4117c19898>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117c19630>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117c1e3c8>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f4117c1eda0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117c1ed68>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117c26ac8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117c2d668>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f4117c2de80>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117c2df98>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117c34c18>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117c3c7b8>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f4117c3c668>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117bc5208>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117bc5d68>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4117bc9908>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f4117bd2390>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYKONB-_Bg0s",
        "colab_type": "text"
      },
      "source": [
        "### Let's add our FC Head back onto VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "050KRlRZBg0t",
        "colab_type": "code",
        "outputId": "f604a732-6600-45d3-eec9-972b9efbb296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "FC_Head = addTopModel(model, num_classes)\n",
        "\n",
        "modelnew = Model(inputs=model.input, outputs=FC_Head)\n",
        "\n",
        "print(modelnew.summary())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1024)              4195328   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 32)                8224      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 122,338,978\n",
            "Trainable params: 107,624,290\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-_dfOwNBg00",
        "colab_type": "text"
      },
      "source": [
        "### Loading our Face Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbuUJJ0mJ2It",
        "colab_type": "code",
        "outputId": "0fb5a3e4-3a51-41ab-fb7b-073d7e5536ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea13dHTABg01",
        "colab_type": "code",
        "outputId": "d78ef872-e5f7-44f3-d8a1-e631302d0c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_dir = 'drive/My Drive/wiki_crop/'\n",
        "validation_data_dir = 'drive/My Drive/a_test/'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=45,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        " \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "# Change the batchsize according to your system RAM\n",
        "batchsize = 32\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batchsize,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batchsize,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2134 images belonging to 2 classes.\n",
            "Found 96 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u_pWennBg1N",
        "colab_type": "text"
      },
      "source": [
        "### Training our top layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcHr04HLBg1O",
        "colab_type": "code",
        "outputId": "95a129d1-3f4f-4d4a-9ebc-7e4ca64615c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "                   \n",
        "checkpoint = ModelCheckpoint(\"face_vgg.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          verbose = 1,\n",
        "                          patience= 3,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "# we put our call backs into a callback list\n",
        "callbacks = [earlystop, checkpoint]\n",
        "\n",
        "# Note we use a very small learning rate \n",
        "modelnew.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = Adam(lr = 0.0001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "nb_train_samples = 2100 \n",
        "nb_validation_samples = 90\n",
        "epochs = 5\n",
        "train_batch_size = 32\n",
        "val_batch_size = 16\n",
        "\n",
        "history = modelnew.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // train_batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // val_batch_size)\n",
        "\n",
        "modelnew.save(\"face_vgg.h5\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "65/65 [==============================] - 31s 471ms/step - loss: 0.4710 - accuracy: 0.7773 - val_loss: 1.1511 - val_accuracy: 0.4563\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.15108, saving model to face_vgg.h5\n",
            "Epoch 2/5\n",
            "65/65 [==============================] - 30s 463ms/step - loss: 0.4728 - accuracy: 0.7696 - val_loss: 1.2750 - val_accuracy: 0.4187\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.15108\n",
            "Epoch 3/5\n",
            "65/65 [==============================] - 30s 455ms/step - loss: 0.4540 - accuracy: 0.7792 - val_loss: 1.4416 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.15108\n",
            "Epoch 4/5\n",
            "65/65 [==============================] - 30s 459ms/step - loss: 0.4601 - accuracy: 0.7763 - val_loss: 1.0420 - val_accuracy: 0.5375\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.15108 to 1.04202, saving model to face_vgg.h5\n",
            "Epoch 5/5\n",
            "65/65 [==============================] - 29s 443ms/step - loss: 0.4525 - accuracy: 0.7792 - val_loss: 0.7206 - val_accuracy: 0.4125\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.04202 to 0.72065, saving model to face_vgg.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCU1O8_v0iLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}